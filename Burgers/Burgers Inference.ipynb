{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtSFXNgoBPb-"
      },
      "source": [
        "# Attribute\n",
        "\n",
        "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
        "\n",
        "**Github Repo** : https://github.com/maziarraissi/PINNs\n",
        "\n",
        "**Link:** https://github.com/maziarraissi/PINNs/tree/master/appendix/continuous_time_identification%20(Burgers)\n",
        "\n",
        "@article{raissi2017physicsI,\n",
        "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10561},\n",
        "  year={2017}\n",
        "}\n",
        "\n",
        "@article{raissi2017physicsII,\n",
        "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10566},\n",
        "  year={2017}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nty94OqjBPcC"
      },
      "source": [
        "## Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vCDHrQyyBPcD"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../Utilities/')\n",
        "\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from pyDOE import lhs\n",
        "import numpy as np\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GUNvzj-ZBPcE"
      },
      "outputs": [],
      "source": [
        "# CUDA support\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQT43UVTBPcF"
      },
      "source": [
        "## Physics-informed Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "USfxO5KqBPcF"
      },
      "outputs": [],
      "source": [
        "# deep neural network\n",
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        self.depth = len(layers) - 1\n",
        "\n",
        "        self.activation = torch.nn.Tanh\n",
        "\n",
        "        layer_list = list()\n",
        "        for i in range(self.depth - 1):\n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "\n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
        "        )\n",
        "        layerDict = OrderedDict(layer_list)\n",
        "\n",
        "        self.layers = torch.nn.Sequential(layerDict)\n",
        "\n",
        "        for module in self.layers.modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                torch.nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gUZx2DzjBPcF"
      },
      "outputs": [],
      "source": [
        "class PhysicsInformedNN():\n",
        "    def __init__(self, X_u, u, X_f, c, all_x, all_t, layers, lb, ub, nu):\n",
        "\n",
        "        # boundary conditions\n",
        "        self.lb = torch.tensor(lb).float().to(device)\n",
        "        self.ub = torch.tensor(ub).float().to(device)\n",
        "\n",
        "        # x and t data for states and collocation points\n",
        "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.all_x = torch.tensor(all_x, requires_grad=True).float().to(device)\n",
        "        self.all_t = torch.tensor(all_t, requires_grad=True).float().to(device)\n",
        "        self.delta_x = 1/128\n",
        "        # state data\n",
        "        self.u = torch.tensor(u).float().to(device)\n",
        "        # conserved quantity\n",
        "        self.c = c\n",
        "\n",
        "        self.layers = layers\n",
        "        self.nu = nu\n",
        "\n",
        "        self.dnn = DNN(layers).to(device)\n",
        "\n",
        "        self.optimizer = torch.optim.LBFGS(\n",
        "            self.dnn.parameters(),\n",
        "            lr=1.0,\n",
        "            max_iter=50000,\n",
        "            max_eval=50000,\n",
        "            history_size=50,\n",
        "            tolerance_grad=1e-5,\n",
        "            tolerance_change=1.0 * np.finfo(float).eps,\n",
        "            line_search_fn=\"strong_wolfe\"\n",
        "        )\n",
        "\n",
        "        self.iter = 0\n",
        "\n",
        "    def net_u(self, x, t):\n",
        "\n",
        "        u = self.dnn(torch.cat([x, t], dim=1))\n",
        "        # projection layer\n",
        "        volume_x = 2\n",
        "\n",
        "        # form mesh containing passed in t values and all x values \n",
        "        mesh_t, mesh_x = torch.meshgrid([t.squeeze(1), self.all_x.squeeze(1)], indexing='ij')\n",
        "        t_by_x = torch.concat((mesh_x.unsqueeze(2), mesh_t.unsqueeze(2)), dim=-1)\n",
        "\n",
        "        # use integral on mesh to find current momentum of system\n",
        "        integral_u_dx = torch.sum(self.dnn(t_by_x)*self.delta_x, dim=1)\n",
        "        second_term = integral_u_dx / volume_x\n",
        "\n",
        "        # create tensor of conserved quantity values\n",
        "        c_tensor = torch.full(x.shape, self.c)\n",
        "        third_term = c_tensor / volume_x\n",
        "\n",
        "        # return result of projection\n",
        "        return u - second_term + third_term\n",
        "        # comment all code above this until 'projection layer' to disable projection layer\n",
        "        return u\n",
        "\n",
        "    def net_f(self, x, t):\n",
        "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
        "        u = self.net_u(x, t)\n",
        "        u_t = torch.autograd.grad(\n",
        "            u, t,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_x = torch.autograd.grad(\n",
        "            u, x,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_xx = torch.autograd.grad(\n",
        "            u_x, x,\n",
        "            grad_outputs=torch.ones_like(u_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "\n",
        "        # burgers equation\n",
        "        f = u_t + u * u_x - self.nu * u_xx\n",
        "        return f\n",
        "\n",
        "    def loss_func(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        u_pred = self.net_u(self.x_u, self.t_u)\n",
        "        f_pred = self.net_f(self.x_f, self.t_f)\n",
        "        loss_u = torch.mean((self.u - u_pred) ** 2)\n",
        "        loss_f = torch.mean(f_pred ** 2)\n",
        "\n",
        "        loss = loss_u + loss_f\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            print(\n",
        "                'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
        "            )\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        self.dnn.train()\n",
        "\n",
        "        self.optimizer.step(self.loss_func)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
        "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        self.dnn.eval()\n",
        "        u = self.net_u(x, t)\n",
        "        f = self.net_f(x, t)\n",
        "        c = torch.sum(u*self.delta_x)\n",
        "        u = u.detach().cpu().numpy()\n",
        "        f = f.detach().cpu().numpy()\n",
        "        c = c.detach().cpu().numpy()\n",
        "        return u, f, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqfVFb6aBPcG"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDvxZz4tBPcH"
      },
      "outputs": [],
      "source": [
        "nu = 0.01/np.pi\n",
        "noise = 0.0\n",
        "\n",
        "N_u = 100\n",
        "N_f = 10000\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "\n",
        "data = scipy.io.loadmat('../data/burgers_shock.mat')\n",
        "\n",
        "t = data['t'].flatten()[:,None]\n",
        "x = data['x'].flatten()[:,None]\n",
        "Exact = np.real(data['usol']).T\n",
        "\n",
        "X, T = np.meshgrid(x,t)\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact.flatten()[:,None]\n",
        "\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "# boundary points\n",
        "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "uu1 = Exact[0:1,:].T\n",
        "xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "uu2 = Exact[:,0:1]\n",
        "xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "uu3 = Exact[:,-1:]\n",
        "\n",
        "X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "# conserved quantity\n",
        "c = np.mean(np.sum(Exact*1/128, axis=1))\n",
        "\n",
        "X_f_train = np.vstack((X_f_train, X_u_train))\n",
        "u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "# take random sample of data\n",
        "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "X_u_train = X_u_train[idx, :]\n",
        "u_train = u_train[idx,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lp3TK1EBPcH"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X_vyIhUnBPcH"
      },
      "outputs": [],
      "source": [
        "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, c, x, t, layers, lb, ub, nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SiaQjMRBPcH",
        "outputId": "5b995e96-2aaa-410a-a703-3f7e5f7587f5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oBHMKejBPcI",
        "outputId": "1b250678-d2a1-435f-bb9f-10c172f85356"
      },
      "outputs": [],
      "source": [
        "u_pred, f_pred, c_pred = model.predict(X_star)\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "\n",
        "c_true = np.mean(np.sum(Exact*1/128, axis=1))\n",
        "error_c = abs(c_pred-c_true)\n",
        "\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error c: %e' % (error_c))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
