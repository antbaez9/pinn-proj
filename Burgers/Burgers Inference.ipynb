{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtSFXNgoBPb-"
      },
      "source": [
        "# Attribute\n",
        "\n",
        "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
        "\n",
        "**Github Repo** : https://github.com/maziarraissi/PINNs\n",
        "\n",
        "**Link:** https://github.com/maziarraissi/PINNs/tree/master/appendix/continuous_time_identification%20(Burgers)\n",
        "\n",
        "@article{raissi2017physicsI,\n",
        "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10561},\n",
        "  year={2017}\n",
        "}\n",
        "\n",
        "@article{raissi2017physicsII,\n",
        "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10566},\n",
        "  year={2017}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nty94OqjBPcC"
      },
      "source": [
        "## Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vCDHrQyyBPcD"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../Utilities/')\n",
        "\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from pyDOE import lhs\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "# from plotting import newfig, savefig\n",
        "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "# import matplotlib.gridspec as gridspec\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GUNvzj-ZBPcE"
      },
      "outputs": [],
      "source": [
        "# CUDA support\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQT43UVTBPcF"
      },
      "source": [
        "## Physics-informed Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "USfxO5KqBPcF"
      },
      "outputs": [],
      "source": [
        "# the deep neural network\n",
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        # parameters\n",
        "        self.depth = len(layers) - 1\n",
        "\n",
        "        # set up layer order dict\n",
        "        self.activation = torch.nn.Tanh\n",
        "\n",
        "        layer_list = list()\n",
        "        for i in range(self.depth - 1):\n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "\n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
        "        )\n",
        "        layerDict = OrderedDict(layer_list)\n",
        "\n",
        "        # deploy layers\n",
        "        self.layers = torch.nn.Sequential(layerDict)\n",
        "\n",
        "        for module in self.layers.modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                torch.nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gUZx2DzjBPcF"
      },
      "outputs": [],
      "source": [
        "# the physics-guided neural network\n",
        "class PhysicsInformedNN():\n",
        "    def __init__(self, X_u, u, X_f, c, all_x, all_t, layers, lb, ub, nu):\n",
        "\n",
        "        # boundary conditions\n",
        "        self.lb = torch.tensor(lb).float().to(device)\n",
        "        self.ub = torch.tensor(ub).float().to(device)\n",
        "\n",
        "        # data\n",
        "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.all_x = torch.tensor(all_x, requires_grad=True).float().to(device)\n",
        "        self.all_t = torch.tensor(all_t, requires_grad=True).float().to(device)\n",
        "        self.u = torch.tensor(u).float().to(device)\n",
        "        self.delta_x = 1/128\n",
        "        self.c = c\n",
        "\n",
        "        self.layers = layers\n",
        "        self.nu = nu\n",
        "\n",
        "        # deep neural networks\n",
        "        self.dnn = DNN(layers).to(device)\n",
        "\n",
        "        # optimizers: using the same settings\n",
        "        self.optimizer = torch.optim.LBFGS(\n",
        "            self.dnn.parameters(),\n",
        "            lr=1.0,\n",
        "            max_iter=50000,\n",
        "            max_eval=50000,\n",
        "            history_size=50,\n",
        "            tolerance_grad=1e-5,\n",
        "            tolerance_change=1.0 * np.finfo(float).eps,\n",
        "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
        "        )\n",
        "\n",
        "        self.iter = 0\n",
        "\n",
        "    def net_u(self, x, t):\n",
        "\n",
        "        u = self.dnn(torch.cat([x, t], dim=1))\n",
        "\n",
        "        volume_x = 2\n",
        "        delta_x = 1/128\n",
        "\n",
        "        mesh_t, mesh_x = torch.meshgrid([t.squeeze(1), self.all_x.squeeze(1)], indexing='ij')\n",
        "        t_by_x = torch.concat((mesh_x.unsqueeze(2), mesh_t.unsqueeze(2)), dim=-1)\n",
        "\n",
        "        integral_u_dx = torch.sum(self.dnn(t_by_x)*delta_x, dim=1)\n",
        "        second_term = integral_u_dx / volume_x\n",
        "\n",
        "        c_tensor = torch.full(x.shape, self.c)\n",
        "        third_term = c_tensor / volume_x\n",
        "        return u - second_term + third_term\n",
        "        return u\n",
        "\n",
        "    def net_f(self, x, t):\n",
        "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
        "        u = self.net_u(x, t)\n",
        "        u_t = torch.autograd.grad(\n",
        "            u, t,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_x = torch.autograd.grad(\n",
        "            u, x,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_xx = torch.autograd.grad(\n",
        "            u_x, x,\n",
        "            grad_outputs=torch.ones_like(u_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "\n",
        "        f = u_t + u * u_x - self.nu * u_xx\n",
        "        return f\n",
        "\n",
        "    def loss_func(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        u_pred = self.net_u(self.x_u, self.t_u)\n",
        "        f_pred = self.net_f(self.x_f, self.t_f)\n",
        "        loss_u = torch.mean((self.u - u_pred) ** 2)\n",
        "        loss_f = torch.mean(f_pred ** 2)\n",
        "\n",
        "        loss = loss_u + loss_f\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            print(\n",
        "                'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
        "            )\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        self.dnn.train()\n",
        "\n",
        "        # Backward and optimize\n",
        "        self.optimizer.step(self.loss_func)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
        "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        self.dnn.eval()\n",
        "        u = self.net_u(x, t)\n",
        "        f = self.net_f(x, t)\n",
        "        c = torch.sum(u*self.delta_x)\n",
        "        u = u.detach().cpu().numpy()\n",
        "        f = f.detach().cpu().numpy()\n",
        "        c = c.detach().cpu().numpy()\n",
        "        return u, f, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqfVFb6aBPcG"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fDvxZz4tBPcH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.2856382625159313e-15\n",
            "[[ 0.04313725  0.        ]\n",
            " [ 1.          0.27      ]\n",
            " [ 1.          0.11      ]\n",
            " [ 1.          0.74      ]\n",
            " [-1.          0.79      ]\n",
            " [-0.39607843  0.        ]\n",
            " [-0.65490196  0.        ]\n",
            " [-0.89019608  0.        ]\n",
            " [-1.          0.94      ]\n",
            " [ 1.          0.88      ]\n",
            " [-0.8745098   0.        ]\n",
            " [ 0.77254902  0.        ]\n",
            " [ 0.50588235  0.        ]\n",
            " [ 0.43529412  0.        ]\n",
            " [-0.27843137  0.        ]\n",
            " [ 1.          0.99      ]\n",
            " [ 1.          0.52      ]\n",
            " [ 1.          0.83      ]\n",
            " [ 1.          0.21      ]\n",
            " [ 0.49803922  0.        ]\n",
            " [ 0.70196078  0.        ]\n",
            " [-1.          0.99      ]\n",
            " [-0.09803922  0.        ]\n",
            " [ 0.71764706  0.        ]\n",
            " [ 1.          0.23      ]\n",
            " [-1.          0.61      ]\n",
            " [ 1.          0.38      ]\n",
            " [-0.21568627  0.        ]\n",
            " [-0.15294118  0.        ]\n",
            " [-1.          0.48      ]\n",
            " [-1.          0.31      ]\n",
            " [ 1.          0.13      ]\n",
            " [ 1.          0.48      ]\n",
            " [ 0.38823529  0.        ]\n",
            " [ 0.34117647  0.        ]\n",
            " [-1.          0.32      ]\n",
            " [ 1.          0.55      ]\n",
            " [ 1.          0.59      ]\n",
            " [ 0.37254902  0.        ]\n",
            " [ 0.34901961  0.        ]\n",
            " [ 0.2627451   0.        ]\n",
            " [ 0.22352941  0.        ]\n",
            " [-0.50588235  0.        ]\n",
            " [ 1.          0.58      ]\n",
            " [ 0.58431373  0.        ]\n",
            " [ 0.7254902   0.        ]\n",
            " [ 0.35686275  0.        ]\n",
            " [ 1.          0.35      ]\n",
            " [ 0.61568627  0.        ]\n",
            " [-1.          0.16      ]\n",
            " [-0.84313725  0.        ]\n",
            " [ 1.          0.36      ]\n",
            " [-1.          0.7       ]\n",
            " [-1.          0.74      ]\n",
            " [ 0.09019608  0.        ]\n",
            " [ 0.10588235  0.        ]\n",
            " [-0.02745098  0.        ]\n",
            " [ 0.39607843  0.        ]\n",
            " [-1.          0.91      ]\n",
            " [-1.          0.69      ]\n",
            " [ 0.99215686  0.        ]\n",
            " [-1.          0.27      ]\n",
            " [-1.          0.85      ]\n",
            " [ 1.          0.78      ]\n",
            " [-0.6         0.        ]\n",
            " [-1.          0.2       ]\n",
            " [ 1.          0.2       ]\n",
            " [ 0.27058824  0.        ]\n",
            " [-0.19215686  0.        ]\n",
            " [-0.45098039  0.        ]\n",
            " [ 0.9372549   0.        ]\n",
            " [-1.          0.9       ]\n",
            " [ 1.          0.08      ]\n",
            " [-1.          0.67      ]\n",
            " [-1.          0.12      ]\n",
            " [-0.06666667  0.        ]\n",
            " [-0.89803922  0.        ]\n",
            " [ 1.          0.65      ]\n",
            " [ 1.          0.16      ]\n",
            " [-1.          0.77      ]\n",
            " [-0.69411765  0.        ]\n",
            " [ 0.63137255  0.        ]\n",
            " [ 0.52156863  0.        ]\n",
            " [ 0.33333333  0.        ]\n",
            " [ 0.44313725  0.        ]\n",
            " [ 1.          0.51      ]\n",
            " [-0.95294118  0.        ]\n",
            " [-0.74117647  0.        ]\n",
            " [-0.62352941  0.        ]\n",
            " [-1.          0.54      ]\n",
            " [-1.          0.09      ]\n",
            " [-1.          0.76      ]\n",
            " [-1.          0.86      ]\n",
            " [ 0.0745098   0.        ]\n",
            " [-0.04313725  0.        ]\n",
            " [ 0.51372549  0.        ]\n",
            " [-0.76470588  0.        ]\n",
            " [ 1.          0.67      ]\n",
            " [ 1.          0.34      ]\n",
            " [ 0.68627451  0.        ]]\n"
          ]
        }
      ],
      "source": [
        "nu = 0.01/np.pi\n",
        "noise = 0.0\n",
        "\n",
        "N_u = 100\n",
        "N_f = 10000\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "\n",
        "data = scipy.io.loadmat('..data/burgers_shock.mat')\n",
        "\n",
        "# Pinn paper state data\n",
        "t = data['t'].flatten()[:,None]\n",
        "x = data['x'].flatten()[:,None]\n",
        "Exact = np.real(data['usol']).T # (100, 256)\n",
        "\n",
        "X, T = np.meshgrid(x,t)\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact.flatten()[:,None]\n",
        "\n",
        "# Domain bounds\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "uu1 = Exact[0:1,:].T\n",
        "xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "uu2 = Exact[:,0:1]\n",
        "xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "uu3 = Exact[:,-1:]\n",
        "\n",
        "\n",
        "X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "c = np.mean(np.sum(Exact*1/128, axis=1))\n",
        "\n",
        "X_f_train = np.vstack((X_f_train, X_u_train))\n",
        "u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "X_u_train = X_u_train[idx, :]\n",
        "u_train = u_train[idx,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lp3TK1EBPcH"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "X_vyIhUnBPcH"
      },
      "outputs": [],
      "source": [
        "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, c, x, t, layers, lb, ub, nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0SiaQjMRBPcH",
        "outputId": "5b995e96-2aaa-410a-a703-3f7e5f7587f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 100, Loss: 6.15777e-02, Loss_u: 4.04590e-02, Loss_f: 2.11187e-02\n",
            "Iter 200, Loss: 1.58252e-02, Loss_u: 9.28122e-03, Loss_f: 6.54397e-03\n",
            "Iter 300, Loss: 4.67856e-03, Loss_u: 2.28598e-03, Loss_f: 2.39258e-03\n",
            "Iter 400, Loss: 2.22543e-03, Loss_u: 1.06356e-03, Loss_f: 1.16186e-03\n",
            "Iter 500, Loss: 1.05869e-03, Loss_u: 3.59107e-04, Loss_f: 6.99582e-04\n",
            "Iter 600, Loss: 5.35939e-04, Loss_u: 1.36422e-04, Loss_f: 3.99517e-04\n",
            "Iter 700, Loss: 2.90302e-04, Loss_u: 4.71486e-05, Loss_f: 2.43154e-04\n",
            "Iter 800, Loss: 1.95125e-04, Loss_u: 3.54913e-05, Loss_f: 1.59634e-04\n",
            "Iter 900, Loss: 1.50165e-04, Loss_u: 2.60595e-05, Loss_f: 1.24106e-04\n",
            "Iter 1000, Loss: 1.16857e-04, Loss_u: 2.03339e-05, Loss_f: 9.65233e-05\n",
            "Iter 1100, Loss: 8.88997e-05, Loss_u: 1.44914e-05, Loss_f: 7.44083e-05\n",
            "Iter 1200, Loss: 7.40303e-05, Loss_u: 1.27982e-05, Loss_f: 6.12321e-05\n",
            "Iter 1300, Loss: 6.00087e-05, Loss_u: 9.15713e-06, Loss_f: 5.08515e-05\n",
            "Iter 1400, Loss: 5.16496e-05, Loss_u: 8.84731e-06, Loss_f: 4.28023e-05\n",
            "Iter 1500, Loss: 4.45398e-05, Loss_u: 7.31813e-06, Loss_f: 3.72216e-05\n",
            "Iter 1600, Loss: 3.88545e-05, Loss_u: 6.34500e-06, Loss_f: 3.25095e-05\n",
            "Iter 1700, Loss: 3.48360e-05, Loss_u: 6.56195e-06, Loss_f: 2.82741e-05\n",
            "Iter 1800, Loss: 3.17434e-05, Loss_u: 6.40348e-06, Loss_f: 2.53399e-05\n",
            "Iter 1900, Loss: 2.95612e-05, Loss_u: 6.18420e-06, Loss_f: 2.33770e-05\n",
            "Iter 2000, Loss: 2.75637e-05, Loss_u: 5.86588e-06, Loss_f: 2.16978e-05\n",
            "Iter 2100, Loss: 2.52429e-05, Loss_u: 6.03951e-06, Loss_f: 1.92034e-05\n",
            "Iter 2200, Loss: 2.28587e-05, Loss_u: 5.90490e-06, Loss_f: 1.69538e-05\n",
            "Iter 2300, Loss: 2.10491e-05, Loss_u: 5.68346e-06, Loss_f: 1.53656e-05\n",
            "Iter 2400, Loss: 1.92876e-05, Loss_u: 5.55180e-06, Loss_f: 1.37358e-05\n",
            "Iter 2500, Loss: 1.75902e-05, Loss_u: 5.30645e-06, Loss_f: 1.22837e-05\n",
            "Iter 2600, Loss: 1.64044e-05, Loss_u: 4.70167e-06, Loss_f: 1.17027e-05\n",
            "Iter 2700, Loss: 1.52953e-05, Loss_u: 4.50990e-06, Loss_f: 1.07854e-05\n",
            "Iter 2800, Loss: 1.41362e-05, Loss_u: 3.83086e-06, Loss_f: 1.03053e-05\n",
            "Iter 2900, Loss: 1.32507e-05, Loss_u: 3.27652e-06, Loss_f: 9.97419e-06\n",
            "Iter 3000, Loss: 1.23976e-05, Loss_u: 2.76460e-06, Loss_f: 9.63304e-06\n",
            "Iter 3100, Loss: 1.13291e-05, Loss_u: 2.41461e-06, Loss_f: 8.91445e-06\n",
            "Iter 3200, Loss: 1.05002e-05, Loss_u: 2.09525e-06, Loss_f: 8.40500e-06\n",
            "Iter 3300, Loss: 9.73242e-06, Loss_u: 1.99150e-06, Loss_f: 7.74092e-06\n",
            "Iter 3400, Loss: 9.11044e-06, Loss_u: 1.90563e-06, Loss_f: 7.20481e-06\n",
            "Iter 3500, Loss: 8.51737e-06, Loss_u: 1.81568e-06, Loss_f: 6.70169e-06\n",
            "Iter 3600, Loss: 7.99456e-06, Loss_u: 1.58657e-06, Loss_f: 6.40800e-06\n",
            "Iter 3700, Loss: 7.51930e-06, Loss_u: 1.42473e-06, Loss_f: 6.09457e-06\n",
            "Iter 3800, Loss: 7.21785e-06, Loss_u: 1.31926e-06, Loss_f: 5.89859e-06\n",
            "Iter 3900, Loss: 6.82330e-06, Loss_u: 1.22754e-06, Loss_f: 5.59576e-06\n",
            "Iter 4000, Loss: 6.44300e-06, Loss_u: 1.18369e-06, Loss_f: 5.25930e-06\n",
            "Iter 4100, Loss: 5.97119e-06, Loss_u: 1.10629e-06, Loss_f: 4.86489e-06\n",
            "Iter 4200, Loss: 5.65230e-06, Loss_u: 9.82348e-07, Loss_f: 4.66995e-06\n",
            "Iter 4300, Loss: 5.38566e-06, Loss_u: 8.67331e-07, Loss_f: 4.51833e-06\n",
            "Iter 4400, Loss: 5.17824e-06, Loss_u: 7.95796e-07, Loss_f: 4.38244e-06\n",
            "Iter 4500, Loss: 4.94568e-06, Loss_u: 7.25128e-07, Loss_f: 4.22055e-06\n",
            "Iter 4600, Loss: 4.75734e-06, Loss_u: 6.68580e-07, Loss_f: 4.08876e-06\n",
            "CPU times: user 4min 35s, sys: 3min 11s, total: 7min 46s\n",
            "Wall time: 4min 23s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8oBHMKejBPcI",
        "outputId": "1b250678-d2a1-435f-bb9f-10c172f85356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error u: 4.553463e-03\n",
            "Error c: 7.546535e-02\n"
          ]
        }
      ],
      "source": [
        "u_pred, f_pred, c_pred = model.predict(X_star)\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "\n",
        "# c_pred is prediction of c on test set, c_test is groundtruth of c on test set\n",
        "c_true = np.mean(np.sum(Exact*1/128, axis=1))\n",
        "\n",
        "error_c = abs(c_pred-c_true)\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error c: %e' % (error_c))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
