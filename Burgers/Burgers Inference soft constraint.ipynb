{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtSFXNgoBPb-"
      },
      "source": [
        "# Attribute\n",
        "\n",
        "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
        "\n",
        "**Github Repo** : https://github.com/maziarraissi/PINNs\n",
        "\n",
        "**Link:** https://github.com/maziarraissi/PINNs/tree/master/appendix/continuous_time_identification%20(Burgers)\n",
        "\n",
        "@article{raissi2017physicsI,\n",
        "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10561},\n",
        "  year={2017}\n",
        "}\n",
        "\n",
        "@article{raissi2017physicsII,\n",
        "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10566},\n",
        "  year={2017}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nty94OqjBPcC"
      },
      "source": [
        "## Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vCDHrQyyBPcD"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../Utilities/')\n",
        "\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from pyDOE import lhs\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "# from plotting import newfig, savefig\n",
        "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "# import matplotlib.gridspec as gridspec\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GUNvzj-ZBPcE"
      },
      "outputs": [],
      "source": [
        "# CUDA support\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQT43UVTBPcF"
      },
      "source": [
        "## Physics-informed Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "USfxO5KqBPcF"
      },
      "outputs": [],
      "source": [
        "# the deep neural network\n",
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        # parameters\n",
        "        self.depth = len(layers) - 1\n",
        "\n",
        "        # set up layer order dict\n",
        "        self.activation = torch.nn.Tanh\n",
        "\n",
        "        layer_list = list()\n",
        "        for i in range(self.depth - 1):\n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "\n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
        "        )\n",
        "        layerDict = OrderedDict(layer_list)\n",
        "\n",
        "        # deploy layers\n",
        "        self.layers = torch.nn.Sequential(layerDict)\n",
        "\n",
        "        for module in self.layers.modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                torch.nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gUZx2DzjBPcF"
      },
      "outputs": [],
      "source": [
        "# the physics-guided neural network\n",
        "class PhysicsInformedNN():\n",
        "    def __init__(self, X_u, u, X_f, c, all_x, all_t, layers, lb, ub, nu):\n",
        "\n",
        "        # boundary conditions\n",
        "        self.lb = torch.tensor(lb).float().to(device)\n",
        "        self.ub = torch.tensor(ub).float().to(device)\n",
        "\n",
        "        # data\n",
        "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.all_x = torch.tensor(all_x, requires_grad=True).float().to(device)\n",
        "        self.all_t = torch.tensor(all_t, requires_grad=True).float().to(device)\n",
        "        self.u = torch.tensor(u).float().to(device)\n",
        "        self.delta_x = 1/128\n",
        "        self.c = c\n",
        "\n",
        "        self.layers = layers\n",
        "        self.nu = nu\n",
        "\n",
        "        # deep neural networks\n",
        "        self.dnn = DNN(layers).to(device)\n",
        "\n",
        "        # optimizers: using the same settings\n",
        "        self.optimizer = torch.optim.LBFGS(\n",
        "            self.dnn.parameters(),\n",
        "            lr=1.0,\n",
        "            max_iter=50000,\n",
        "            max_eval=50000,\n",
        "            history_size=50,\n",
        "            tolerance_grad=1e-5,\n",
        "            tolerance_change=1.0 * np.finfo(float).eps,\n",
        "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
        "        )\n",
        "\n",
        "        self.iter = 0\n",
        "\n",
        "    def net_u(self, x, t):\n",
        "\n",
        "        u = self.dnn(torch.cat([x, t], dim=1))\n",
        "\n",
        "        volume_x = 2\n",
        "        delta_x = 1/128\n",
        "\n",
        "        mesh_t, mesh_x = torch.meshgrid([t.squeeze(1), self.all_x.squeeze(1)], indexing='ij')\n",
        "        t_by_x = torch.concat((mesh_x.unsqueeze(2), mesh_t.unsqueeze(2)), dim=-1)\n",
        "\n",
        "        integral_u_dx = torch.sum(self.dnn(t_by_x)*delta_x, dim=1)\n",
        "        second_term = integral_u_dx / volume_x\n",
        "\n",
        "        c_tensor = torch.full(x.shape, self.c)\n",
        "        third_term = c_tensor / volume_x\n",
        "        # return u - second_term + third_term\n",
        "        loss_c = torch.mean((second_term - third_term) ** 2)\n",
        "        return u, loss_c\n",
        "\n",
        "    def net_f(self, x, t):\n",
        "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
        "        u = self.net_u(x, t)[0]\n",
        "        u_t = torch.autograd.grad(\n",
        "            u, t,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_x = torch.autograd.grad(\n",
        "            u, x,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_xx = torch.autograd.grad(\n",
        "            u_x, x,\n",
        "            grad_outputs=torch.ones_like(u_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "\n",
        "        f = u_t + u * u_x - self.nu * u_xx\n",
        "        return f\n",
        "\n",
        "    def loss_func(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        u_pred, loss_c = self.net_u(self.x_u, self.t_u)\n",
        "        f_pred = self.net_f(self.x_f, self.t_f)\n",
        "        loss_u = torch.mean((self.u - u_pred) ** 2)\n",
        "        loss_f = torch.mean(f_pred ** 2)\n",
        "\n",
        "        loss = loss_u + loss_f + 10*loss_c\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            print(\n",
        "                'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
        "            )\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        self.dnn.train()\n",
        "\n",
        "        # Backward and optimize\n",
        "        self.optimizer.step(self.loss_func)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
        "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        self.dnn.eval()\n",
        "        u = self.net_u(x, t)[0]\n",
        "        f = self.net_f(x, t)\n",
        "        c = torch.sum(u*self.delta_x)\n",
        "        u = u.detach().cpu().numpy()\n",
        "        f = f.detach().cpu().numpy()\n",
        "        c = c.detach().cpu().numpy()\n",
        "        return u, f, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqfVFb6aBPcG"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fDvxZz4tBPcH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1.          0.85      ]\n",
            " [-0.16078431  0.        ]\n",
            " [ 0.24705882  0.        ]\n",
            " [ 0.12156863  0.        ]\n",
            " [ 0.90588235  0.        ]\n",
            " [-0.52156863  0.        ]\n",
            " [-1.          0.07      ]\n",
            " [ 0.42745098  0.        ]\n",
            " [ 0.33333333  0.        ]\n",
            " [ 0.63137255  0.        ]\n",
            " [-0.78039216  0.        ]\n",
            " [-0.24705882  0.        ]\n",
            " [ 0.64705882  0.        ]\n",
            " [ 0.39607843  0.        ]\n",
            " [-0.89803922  0.        ]\n",
            " [ 0.00392157  0.        ]\n",
            " [-0.46666667  0.        ]\n",
            " [ 1.          0.72      ]\n",
            " [-0.49803922  0.        ]\n",
            " [ 1.          0.58      ]\n",
            " [-0.3254902   0.        ]\n",
            " [-1.          0.        ]\n",
            " [ 0.36470588  0.        ]\n",
            " [-1.          0.62      ]\n",
            " [ 0.94509804  0.        ]\n",
            " [ 1.          0.77      ]\n",
            " [ 0.19215686  0.        ]\n",
            " [ 0.67058824  0.        ]\n",
            " [-0.90588235  0.        ]\n",
            " [ 0.89803922  0.        ]\n",
            " [-1.          0.99      ]\n",
            " [-1.          0.09      ]\n",
            " [-0.41176471  0.        ]\n",
            " [ 1.          0.54      ]\n",
            " [ 0.70196078  0.        ]\n",
            " [ 0.65490196  0.        ]\n",
            " [ 0.31764706  0.        ]\n",
            " [ 1.          0.14      ]\n",
            " [-0.08235294  0.        ]\n",
            " [ 1.          0.61      ]\n",
            " [-0.6         0.        ]\n",
            " [-1.          0.3       ]\n",
            " [-1.          0.23      ]\n",
            " [-1.          0.87      ]\n",
            " [-0.8745098   0.        ]\n",
            " [-0.99215686  0.        ]\n",
            " [ 1.          0.37      ]\n",
            " [ 1.          0.24      ]\n",
            " [-0.71764706  0.        ]\n",
            " [ 0.97647059  0.        ]\n",
            " [-1.          0.98      ]\n",
            " [-1.          0.96      ]\n",
            " [-1.          0.08      ]\n",
            " [ 0.95294118  0.        ]\n",
            " [ 0.96078431  0.        ]\n",
            " [ 1.          0.44      ]\n",
            " [ 1.          0.84      ]\n",
            " [ 0.76470588  0.        ]\n",
            " [ 1.          0.43      ]\n",
            " [-0.30196078  0.        ]\n",
            " [ 0.56078431  0.        ]\n",
            " [-0.62352941  0.        ]\n",
            " [-0.78823529  0.        ]\n",
            " [-0.12941176  0.        ]\n",
            " [ 0.2627451   0.        ]\n",
            " [-0.58431373  0.        ]\n",
            " [-0.94509804  0.        ]\n",
            " [-1.          0.45      ]\n",
            " [-0.42745098  0.        ]\n",
            " [-1.          0.        ]\n",
            " [-0.67058824  0.        ]\n",
            " [-1.          0.15      ]\n",
            " [ 0.08235294  0.        ]\n",
            " [-1.          0.8       ]\n",
            " [-1.          0.94      ]\n",
            " [-0.54509804  0.        ]\n",
            " [-0.74901961  0.        ]\n",
            " [-0.21568627  0.        ]\n",
            " [ 0.51372549  0.        ]\n",
            " [-1.          0.21      ]\n",
            " [-1.          0.76      ]\n",
            " [ 1.          0.32      ]\n",
            " [-0.5372549   0.        ]\n",
            " [-0.75686275  0.        ]\n",
            " [-0.34901961  0.        ]\n",
            " [ 0.74117647  0.        ]\n",
            " [ 1.          0.46      ]\n",
            " [-1.          0.66      ]\n",
            " [-0.05882353  0.        ]\n",
            " [-1.          0.72      ]\n",
            " [-0.70980392  0.        ]\n",
            " [-1.          0.14      ]\n",
            " [-0.28627451  0.        ]\n",
            " [-0.85882353  0.        ]\n",
            " [-1.          0.05      ]\n",
            " [-1.          0.61      ]\n",
            " [-1.          0.18      ]\n",
            " [-1.          0.39      ]\n",
            " [ 0.01960784  0.        ]\n",
            " [-1.          0.35      ]]\n"
          ]
        }
      ],
      "source": [
        "nu = 0.01/np.pi\n",
        "noise = 0.0\n",
        "\n",
        "N_u = 100\n",
        "N_f = 10000\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "\n",
        "data = scipy.io.loadmat('../data/burgers_shock.mat')\n",
        "\n",
        "# Pinn paper state data\n",
        "t = data['t'].flatten()[:,None]\n",
        "x = data['x'].flatten()[:,None]\n",
        "Exact = np.real(data['usol']).T # (100, 256)\n",
        "\n",
        "X, T = np.meshgrid(x,t)\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact.flatten()[:,None]\n",
        "\n",
        "# Domain bounds\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "uu1 = Exact[0:1,:].T\n",
        "xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "uu2 = Exact[:,0:1]\n",
        "xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "uu3 = Exact[:,-1:]\n",
        "\n",
        "\n",
        "X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "c = np.mean(np.sum(Exact*1/128, axis=1))\n",
        "\n",
        "X_f_train = np.vstack((X_f_train, X_u_train))\n",
        "u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "X_u_train = X_u_train[idx, :]\n",
        "u_train = u_train[idx,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lp3TK1EBPcH"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "X_vyIhUnBPcH"
      },
      "outputs": [],
      "source": [
        "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, c, x, t, layers, lb, ub, nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0SiaQjMRBPcH",
        "outputId": "5b995e96-2aaa-410a-a703-3f7e5f7587f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 100, Loss: 6.88249e-02, Loss_u: 4.41407e-02, Loss_f: 2.34157e-02\n",
            "Iter 200, Loss: 4.60386e-02, Loss_u: 2.86332e-02, Loss_f: 1.58805e-02\n",
            "Iter 300, Loss: 2.83672e-02, Loss_u: 1.52407e-02, Loss_f: 1.15915e-02\n",
            "Iter 400, Loss: 1.26709e-02, Loss_u: 4.34006e-03, Loss_f: 6.31683e-03\n",
            "Iter 500, Loss: 7.30118e-03, Loss_u: 2.30157e-03, Loss_f: 3.36599e-03\n",
            "Iter 600, Loss: 4.58234e-03, Loss_u: 1.53072e-03, Loss_f: 1.71337e-03\n",
            "Iter 700, Loss: 3.94054e-03, Loss_u: 1.21012e-03, Loss_f: 1.42090e-03\n",
            "Iter 800, Loss: 3.44209e-03, Loss_u: 1.09732e-03, Loss_f: 1.07123e-03\n",
            "Iter 900, Loss: 3.00812e-03, Loss_u: 8.93631e-04, Loss_f: 1.01569e-03\n",
            "Iter 1000, Loss: 2.65310e-03, Loss_u: 8.46390e-04, Loss_f: 7.22348e-04\n",
            "Iter 1100, Loss: 2.50710e-03, Loss_u: 7.82612e-04, Loss_f: 6.51597e-04\n",
            "Iter 1200, Loss: 2.39162e-03, Loss_u: 7.66060e-04, Loss_f: 5.56017e-04\n",
            "Iter 1300, Loss: 2.23831e-03, Loss_u: 7.15010e-04, Loss_f: 4.72760e-04\n",
            "Iter 1400, Loss: 2.12168e-03, Loss_u: 7.02677e-04, Loss_f: 3.94436e-04\n",
            "Iter 1500, Loss: 2.02717e-03, Loss_u: 6.52109e-04, Loss_f: 3.34586e-04\n",
            "Iter 1600, Loss: 1.97446e-03, Loss_u: 6.25659e-04, Loss_f: 3.06006e-04\n",
            "Iter 1700, Loss: 1.92603e-03, Loss_u: 6.32815e-04, Loss_f: 2.63998e-04\n",
            "Iter 1800, Loss: 1.87923e-03, Loss_u: 5.93424e-04, Loss_f: 2.42645e-04\n",
            "Iter 1900, Loss: 1.83357e-03, Loss_u: 5.94628e-04, Loss_f: 2.29866e-04\n",
            "Iter 2000, Loss: 1.77641e-03, Loss_u: 6.71026e-04, Loss_f: 1.72297e-04\n",
            "Iter 2100, Loss: 1.74259e-03, Loss_u: 6.20051e-04, Loss_f: 1.64131e-04\n",
            "Iter 2200, Loss: 1.70309e-03, Loss_u: 5.96459e-04, Loss_f: 1.40122e-04\n",
            "Iter 2300, Loss: 1.68166e-03, Loss_u: 6.07236e-04, Loss_f: 1.23456e-04\n",
            "Iter 2400, Loss: 1.66612e-03, Loss_u: 5.90163e-04, Loss_f: 1.10678e-04\n",
            "Iter 2500, Loss: 1.65398e-03, Loss_u: 5.75055e-04, Loss_f: 1.02534e-04\n",
            "Iter 2600, Loss: 1.64343e-03, Loss_u: 5.71701e-04, Loss_f: 8.95505e-05\n",
            "Iter 2700, Loss: 1.63481e-03, Loss_u: 5.68711e-04, Loss_f: 8.34951e-05\n",
            "Iter 2800, Loss: 1.62462e-03, Loss_u: 5.67205e-04, Loss_f: 7.41588e-05\n",
            "Iter 2900, Loss: 1.61152e-03, Loss_u: 5.63761e-04, Loss_f: 6.72444e-05\n",
            "Iter 3000, Loss: 1.60523e-03, Loss_u: 5.58366e-04, Loss_f: 6.34051e-05\n",
            "Iter 3100, Loss: 1.59991e-03, Loss_u: 5.58267e-04, Loss_f: 6.08166e-05\n",
            "Iter 3200, Loss: 1.59366e-03, Loss_u: 5.61542e-04, Loss_f: 5.63328e-05\n",
            "Iter 3300, Loss: 1.58747e-03, Loss_u: 5.61060e-04, Loss_f: 5.18236e-05\n",
            "Iter 3400, Loss: 1.58328e-03, Loss_u: 5.61989e-04, Loss_f: 4.80100e-05\n",
            "Iter 3500, Loss: 1.58011e-03, Loss_u: 5.62749e-04, Loss_f: 4.48865e-05\n",
            "Iter 3600, Loss: 1.57625e-03, Loss_u: 5.57705e-04, Loss_f: 4.19671e-05\n",
            "Iter 3700, Loss: 1.57366e-03, Loss_u: 5.64428e-04, Loss_f: 3.98306e-05\n",
            "Iter 3800, Loss: 1.57119e-03, Loss_u: 5.61942e-04, Loss_f: 3.85868e-05\n",
            "Iter 3900, Loss: 1.56901e-03, Loss_u: 5.63747e-04, Loss_f: 3.74928e-05\n",
            "Iter 4000, Loss: 1.56733e-03, Loss_u: 5.67174e-04, Loss_f: 3.57717e-05\n",
            "Iter 4100, Loss: 1.56543e-03, Loss_u: 5.61268e-04, Loss_f: 3.35624e-05\n",
            "Iter 4200, Loss: 1.56321e-03, Loss_u: 5.61386e-04, Loss_f: 3.32732e-05\n",
            "Iter 4300, Loss: 1.56145e-03, Loss_u: 5.60160e-04, Loss_f: 3.14167e-05\n",
            "Iter 4400, Loss: 1.55926e-03, Loss_u: 5.59478e-04, Loss_f: 2.91641e-05\n",
            "Iter 4500, Loss: 1.55727e-03, Loss_u: 5.57512e-04, Loss_f: 2.63962e-05\n",
            "Iter 4600, Loss: 1.55588e-03, Loss_u: 5.57315e-04, Loss_f: 2.48264e-05\n",
            "Iter 4700, Loss: 1.55498e-03, Loss_u: 5.57098e-04, Loss_f: 2.39969e-05\n",
            "Iter 4800, Loss: 1.55398e-03, Loss_u: 5.57215e-04, Loss_f: 2.32672e-05\n",
            "Iter 4900, Loss: 1.55294e-03, Loss_u: 5.58131e-04, Loss_f: 2.30697e-05\n",
            "Iter 5000, Loss: 1.55220e-03, Loss_u: 5.56776e-04, Loss_f: 2.23944e-05\n",
            "Iter 5100, Loss: 1.55155e-03, Loss_u: 5.59913e-04, Loss_f: 2.23329e-05\n",
            "CPU times: user 3min 55s, sys: 2min 40s, total: 6min 35s\n",
            "Wall time: 3min 49s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8oBHMKejBPcI",
        "outputId": "1b250678-d2a1-435f-bb9f-10c172f85356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error u: 1.614900e-01\n",
            "Error c: 4.922191e+00\n"
          ]
        }
      ],
      "source": [
        "u_pred, f_pred, c_pred = model.predict(X_star)\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "\n",
        "# c_pred is prediction of c on test set, c_test is groundtruth of c on test set\n",
        "c_true = np.mean(np.sum(Exact*1/128, axis=1))\n",
        "\n",
        "error_c = abs(c_pred-c_true)\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error c: %e' % (error_c))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
