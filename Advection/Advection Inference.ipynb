{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtSFXNgoBPb-"
      },
      "source": [
        "# Attribute\n",
        "\n",
        "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
        "\n",
        "**Github Repo** : https://github.com/maziarraissi/PINNs\n",
        "\n",
        "**Link:** https://github.com/maziarraissi/PINNs/tree/master/appendix/continuous_time_identification%20(Burgers)\n",
        "\n",
        "@article{raissi2017physicsI,\n",
        "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10561},\n",
        "  year={2017}\n",
        "}\n",
        "\n",
        "@article{raissi2017physicsII,\n",
        "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10566},\n",
        "  year={2017}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nty94OqjBPcC"
      },
      "source": [
        "## Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "vCDHrQyyBPcD"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../Utilities/')\n",
        "\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from pyDOE import lhs\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "# from plotting import newfig, savefig\n",
        "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "# import matplotlib.gridspec as gridspec\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "GUNvzj-ZBPcE"
      },
      "outputs": [],
      "source": [
        "# CUDA support\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQT43UVTBPcF"
      },
      "source": [
        "## Physics-informed Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "id": "USfxO5KqBPcF"
      },
      "outputs": [],
      "source": [
        "# the deep neural network\n",
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        # parameters\n",
        "        self.depth = len(layers) - 1\n",
        "\n",
        "        # set up layer order dict\n",
        "        self.activation = torch.nn.Tanh\n",
        "\n",
        "        layer_list = list()\n",
        "        for i in range(self.depth - 1):\n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "\n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
        "        )\n",
        "        layerDict = OrderedDict(layer_list)\n",
        "\n",
        "        # deploy layers\n",
        "        self.layers = torch.nn.Sequential(layerDict)\n",
        "\n",
        "        for module in self.layers.modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                torch.nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "id": "gUZx2DzjBPcF"
      },
      "outputs": [],
      "source": [
        "# the physics-guided neural network\n",
        "class PhysicsInformedNN():\n",
        "    def __init__(self, X_u, u, X_f, c, all_x, all_t, layers, lb, ub, nu):\n",
        "\n",
        "        # boundary conditions\n",
        "        self.lb = torch.tensor(lb).float().to(device)\n",
        "        self.ub = torch.tensor(ub).float().to(device)\n",
        "\n",
        "        # data\n",
        "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.all_x = torch.tensor(all_x, requires_grad=True).float().to(device)\n",
        "        self.all_t = torch.tensor(all_t, requires_grad=True).float().to(device)\n",
        "        self.u = torch.tensor(u).float().to(device)\n",
        "        self.delta_x = 1/128\n",
        "        self.c = c\n",
        "\n",
        "        self.layers = layers\n",
        "        self.nu = nu\n",
        "\n",
        "        # deep neural networks\n",
        "        self.dnn = DNN(layers).to(device)\n",
        "\n",
        "        # optimizers: using the same settings\n",
        "        self.optimizer = torch.optim.LBFGS(\n",
        "            self.dnn.parameters(),\n",
        "            lr=1.0,\n",
        "            max_iter=50000,\n",
        "            max_eval=50000,\n",
        "            history_size=50,\n",
        "            tolerance_grad=1e-5,\n",
        "            tolerance_change=1.0 * np.finfo(float).eps,\n",
        "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
        "        )\n",
        "\n",
        "        self.iter = 0\n",
        "\n",
        "    def net_u(self, x, t):\n",
        "\n",
        "        u = self.dnn(torch.cat([x, t], dim=1))\n",
        "\n",
        "        volume_x = 2\n",
        "        delta_x = 1/128\n",
        "\n",
        "        mesh_t, mesh_x = torch.meshgrid([t.squeeze(1), self.all_x.squeeze(1)], indexing='ij')\n",
        "        t_by_x = torch.concat((mesh_x.unsqueeze(2), mesh_t.unsqueeze(2)), dim=-1)\n",
        "\n",
        "        integral_u_dx = torch.sum(self.dnn(t_by_x)*delta_x, dim=1)\n",
        "        second_term = integral_u_dx / volume_x\n",
        "\n",
        "        c_tensor = torch.full(x.shape, self.c)\n",
        "        third_term = c_tensor / volume_x\n",
        "        return u - second_term + third_term\n",
        "        return u\n",
        "\n",
        "    def net_f(self, x, t):\n",
        "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
        "        u = self.net_u(x, t)\n",
        "        u_t = torch.autograd.grad(\n",
        "            u, t,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_x = torch.autograd.grad(\n",
        "            u, x,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_xx = torch.autograd.grad(\n",
        "            u_x, x,\n",
        "            grad_outputs=torch.ones_like(u_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "\n",
        "        f = u_t + self.nu * u_x\n",
        "        return f\n",
        "\n",
        "    def loss_func(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        u_pred = self.net_u(self.x_u, self.t_u)\n",
        "        f_pred = self.net_f(self.x_f, self.t_f)\n",
        "        loss_u = torch.mean((self.u - u_pred) ** 2)\n",
        "        loss_f = torch.mean(f_pred ** 2)\n",
        "\n",
        "        loss = loss_u + loss_f\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            print(\n",
        "                'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
        "            )\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        self.dnn.train()\n",
        "\n",
        "        # Backward and optimize\n",
        "        self.optimizer.step(self.loss_func)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
        "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        self.dnn.eval()\n",
        "        u = self.net_u(x, t)\n",
        "        f = self.net_f(x, t)\n",
        "        c = torch.sum(u*self.delta_x)\n",
        "        u = u.detach().cpu().numpy()\n",
        "        f = f.detach().cpu().numpy()\n",
        "        c = c.detach().cpu().numpy()\n",
        "        return u, f, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqfVFb6aBPcG"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "fDvxZz4tBPcH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1.7162238e-08\n"
          ]
        }
      ],
      "source": [
        "nu = 0.1\n",
        "noise = 0.0\n",
        "\n",
        "N_u = 100\n",
        "N_f = 10000\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "\n",
        "# Phiflow state data\n",
        "t = np.load(\"../data/advection/t_coordinate.npy\")[:-1][:, np.newaxis]\n",
        "x = np.load(\"../data/advection/x_coordinate.npy\")[:256, np.newaxis]\n",
        "Exact = np.load(\"../data/advection/Advection_beta0.1.npy\")\n",
        "\n",
        "X, T = np.meshgrid(x,t)\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = Exact.flatten()[:,None]\n",
        "\n",
        "# Doman bounds\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "\n",
        "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "uu1 = Exact[0:1,:].T\n",
        "xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "\n",
        "uu2 = Exact[:,0:1]\n",
        "xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "uu3 = Exact[:,-1:]\n",
        "\n",
        "X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "c = np.mean(np.sum(Exact*1/128, axis=1))\n",
        "\n",
        "X_f_train = np.vstack((X_f_train, X_u_train))\n",
        "u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "X_u_train = X_u_train[idx, :]\n",
        "u_train = u_train[idx, :]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lp3TK1EBPcH"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {
        "id": "X_vyIhUnBPcH"
      },
      "outputs": [],
      "source": [
        "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, c, x, t, layers, lb, ub, nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "id": "0SiaQjMRBPcH",
        "outputId": "5b995e96-2aaa-410a-a703-3f7e5f7587f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "allx tensor([[0.0000],\n",
            "        [0.0078],\n",
            "        [0.0156],\n",
            "        [0.0234],\n",
            "        [0.0312],\n",
            "        [0.0391],\n",
            "        [0.0469],\n",
            "        [0.0547],\n",
            "        [0.0625],\n",
            "        [0.0703],\n",
            "        [0.0781],\n",
            "        [0.0859],\n",
            "        [0.0938],\n",
            "        [0.1016],\n",
            "        [0.1094],\n",
            "        [0.1172],\n",
            "        [0.1250],\n",
            "        [0.1328],\n",
            "        [0.1406],\n",
            "        [0.1484],\n",
            "        [0.1562],\n",
            "        [0.1641],\n",
            "        [0.1719],\n",
            "        [0.1797],\n",
            "        [0.1875],\n",
            "        [0.1953],\n",
            "        [0.2031],\n",
            "        [0.2109],\n",
            "        [0.2188],\n",
            "        [0.2266],\n",
            "        [0.2344],\n",
            "        [0.2422],\n",
            "        [0.2500],\n",
            "        [0.2578],\n",
            "        [0.2656],\n",
            "        [0.2734],\n",
            "        [0.2812],\n",
            "        [0.2891],\n",
            "        [0.2969],\n",
            "        [0.3047],\n",
            "        [0.3125],\n",
            "        [0.3203],\n",
            "        [0.3281],\n",
            "        [0.3359],\n",
            "        [0.3438],\n",
            "        [0.3516],\n",
            "        [0.3594],\n",
            "        [0.3672],\n",
            "        [0.3750],\n",
            "        [0.3828],\n",
            "        [0.3906],\n",
            "        [0.3984],\n",
            "        [0.4062],\n",
            "        [0.4141],\n",
            "        [0.4219],\n",
            "        [0.4297],\n",
            "        [0.4375],\n",
            "        [0.4453],\n",
            "        [0.4531],\n",
            "        [0.4609],\n",
            "        [0.4688],\n",
            "        [0.4766],\n",
            "        [0.4844],\n",
            "        [0.4922],\n",
            "        [0.5000],\n",
            "        [0.5078],\n",
            "        [0.5156],\n",
            "        [0.5234],\n",
            "        [0.5312],\n",
            "        [0.5391],\n",
            "        [0.5469],\n",
            "        [0.5547],\n",
            "        [0.5625],\n",
            "        [0.5703],\n",
            "        [0.5781],\n",
            "        [0.5859],\n",
            "        [0.5938],\n",
            "        [0.6016],\n",
            "        [0.6094],\n",
            "        [0.6172],\n",
            "        [0.6250],\n",
            "        [0.6328],\n",
            "        [0.6406],\n",
            "        [0.6484],\n",
            "        [0.6562],\n",
            "        [0.6641],\n",
            "        [0.6719],\n",
            "        [0.6797],\n",
            "        [0.6875],\n",
            "        [0.6953],\n",
            "        [0.7031],\n",
            "        [0.7109],\n",
            "        [0.7188],\n",
            "        [0.7266],\n",
            "        [0.7344],\n",
            "        [0.7422],\n",
            "        [0.7500],\n",
            "        [0.7578],\n",
            "        [0.7656],\n",
            "        [0.7734],\n",
            "        [0.7812],\n",
            "        [0.7891],\n",
            "        [0.7969],\n",
            "        [0.8047],\n",
            "        [0.8125],\n",
            "        [0.8203],\n",
            "        [0.8281],\n",
            "        [0.8359],\n",
            "        [0.8438],\n",
            "        [0.8516],\n",
            "        [0.8594],\n",
            "        [0.8672],\n",
            "        [0.8750],\n",
            "        [0.8828],\n",
            "        [0.8906],\n",
            "        [0.8984],\n",
            "        [0.9062],\n",
            "        [0.9141],\n",
            "        [0.9219],\n",
            "        [0.9297],\n",
            "        [0.9375],\n",
            "        [0.9453],\n",
            "        [0.9531],\n",
            "        [0.9609],\n",
            "        [0.9688],\n",
            "        [0.9766],\n",
            "        [0.9844],\n",
            "        [0.9922],\n",
            "        [1.0000],\n",
            "        [1.0078],\n",
            "        [1.0156],\n",
            "        [1.0234],\n",
            "        [1.0312],\n",
            "        [1.0391],\n",
            "        [1.0469],\n",
            "        [1.0547],\n",
            "        [1.0625],\n",
            "        [1.0703],\n",
            "        [1.0781],\n",
            "        [1.0859],\n",
            "        [1.0938],\n",
            "        [1.1016],\n",
            "        [1.1094],\n",
            "        [1.1172],\n",
            "        [1.1250],\n",
            "        [1.1328],\n",
            "        [1.1406],\n",
            "        [1.1484],\n",
            "        [1.1562],\n",
            "        [1.1641],\n",
            "        [1.1719],\n",
            "        [1.1797],\n",
            "        [1.1875],\n",
            "        [1.1953],\n",
            "        [1.2031],\n",
            "        [1.2109],\n",
            "        [1.2188],\n",
            "        [1.2266],\n",
            "        [1.2344],\n",
            "        [1.2422],\n",
            "        [1.2500],\n",
            "        [1.2578],\n",
            "        [1.2656],\n",
            "        [1.2734],\n",
            "        [1.2812],\n",
            "        [1.2891],\n",
            "        [1.2969],\n",
            "        [1.3047],\n",
            "        [1.3125],\n",
            "        [1.3203],\n",
            "        [1.3281],\n",
            "        [1.3359],\n",
            "        [1.3438],\n",
            "        [1.3516],\n",
            "        [1.3594],\n",
            "        [1.3672],\n",
            "        [1.3750],\n",
            "        [1.3828],\n",
            "        [1.3906],\n",
            "        [1.3984],\n",
            "        [1.4062],\n",
            "        [1.4141],\n",
            "        [1.4219],\n",
            "        [1.4297],\n",
            "        [1.4375],\n",
            "        [1.4453],\n",
            "        [1.4531],\n",
            "        [1.4609],\n",
            "        [1.4688],\n",
            "        [1.4766],\n",
            "        [1.4844],\n",
            "        [1.4922],\n",
            "        [1.5000],\n",
            "        [1.5078],\n",
            "        [1.5156],\n",
            "        [1.5234],\n",
            "        [1.5312],\n",
            "        [1.5391],\n",
            "        [1.5469],\n",
            "        [1.5547],\n",
            "        [1.5625],\n",
            "        [1.5703],\n",
            "        [1.5781],\n",
            "        [1.5859],\n",
            "        [1.5938],\n",
            "        [1.6016],\n",
            "        [1.6094],\n",
            "        [1.6172],\n",
            "        [1.6250],\n",
            "        [1.6328],\n",
            "        [1.6406],\n",
            "        [1.6484],\n",
            "        [1.6562],\n",
            "        [1.6641],\n",
            "        [1.6719],\n",
            "        [1.6797],\n",
            "        [1.6875],\n",
            "        [1.6953],\n",
            "        [1.7031],\n",
            "        [1.7109],\n",
            "        [1.7188],\n",
            "        [1.7266],\n",
            "        [1.7344],\n",
            "        [1.7422],\n",
            "        [1.7500],\n",
            "        [1.7578],\n",
            "        [1.7656],\n",
            "        [1.7734],\n",
            "        [1.7812],\n",
            "        [1.7891],\n",
            "        [1.7969],\n",
            "        [1.8047],\n",
            "        [1.8125],\n",
            "        [1.8203],\n",
            "        [1.8281],\n",
            "        [1.8359],\n",
            "        [1.8438],\n",
            "        [1.8516],\n",
            "        [1.8594],\n",
            "        [1.8672],\n",
            "        [1.8750],\n",
            "        [1.8828],\n",
            "        [1.8906],\n",
            "        [1.8984],\n",
            "        [1.9062],\n",
            "        [1.9141],\n",
            "        [1.9219],\n",
            "        [1.9297],\n",
            "        [1.9375],\n",
            "        [1.9453],\n",
            "        [1.9531],\n",
            "        [1.9609],\n",
            "        [1.9688],\n",
            "        [1.9766],\n",
            "        [1.9844],\n",
            "        [1.9922]], requires_grad=True) tensor([[0.0000],\n",
            "        [0.8400],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.7000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.3300],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.4400],\n",
            "        [0.1200],\n",
            "        [0.7500],\n",
            "        [0.0000],\n",
            "        [0.1800],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.3500],\n",
            "        [0.1000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.3800],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.4200],\n",
            "        [0.0000],\n",
            "        [0.2800],\n",
            "        [0.7000],\n",
            "        [0.8000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.8400],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.7900],\n",
            "        [0.2600],\n",
            "        [0.0800],\n",
            "        [0.0000],\n",
            "        [0.5900],\n",
            "        [0.0000],\n",
            "        [0.1400],\n",
            "        [0.7600],\n",
            "        [0.0200],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.5700],\n",
            "        [0.0000],\n",
            "        [0.0900],\n",
            "        [0.8900],\n",
            "        [0.3200],\n",
            "        [0.2700],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.6800],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.4100],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.2200],\n",
            "        [0.2000],\n",
            "        [0.7100],\n",
            "        [0.0000],\n",
            "        [0.9700],\n",
            "        [0.0000],\n",
            "        [0.2900],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.0000],\n",
            "        [0.1500],\n",
            "        [0.2600],\n",
            "        [0.0000],\n",
            "        [0.7400],\n",
            "        [0.1900],\n",
            "        [0.7700],\n",
            "        [0.0000],\n",
            "        [0.3100],\n",
            "        [0.0000]], requires_grad=True)\n",
            "mesh tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.8400, 0.8400, 0.8400,  ..., 0.8400, 0.8400, 0.8400],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        ...,\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
            "        [0.3100, 0.3100, 0.3100,  ..., 0.3100, 0.3100, 0.3100],\n",
            "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<ExpandBackward0>)\n",
            "tbyx tensor([[[0.0000, 0.0000],\n",
            "         [0.0078, 0.0000],\n",
            "         [0.0156, 0.0000],\n",
            "         ...,\n",
            "         [1.9766, 0.0000],\n",
            "         [1.9844, 0.0000],\n",
            "         [1.9922, 0.0000]],\n",
            "\n",
            "        [[0.0000, 0.8400],\n",
            "         [0.0078, 0.8400],\n",
            "         [0.0156, 0.8400],\n",
            "         ...,\n",
            "         [1.9766, 0.8400],\n",
            "         [1.9844, 0.8400],\n",
            "         [1.9922, 0.8400]],\n",
            "\n",
            "        [[0.0000, 0.0000],\n",
            "         [0.0078, 0.0000],\n",
            "         [0.0156, 0.0000],\n",
            "         ...,\n",
            "         [1.9766, 0.0000],\n",
            "         [1.9844, 0.0000],\n",
            "         [1.9922, 0.0000]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.0000, 0.0000],\n",
            "         [0.0078, 0.0000],\n",
            "         [0.0156, 0.0000],\n",
            "         ...,\n",
            "         [1.9766, 0.0000],\n",
            "         [1.9844, 0.0000],\n",
            "         [1.9922, 0.0000]],\n",
            "\n",
            "        [[0.0000, 0.3100],\n",
            "         [0.0078, 0.3100],\n",
            "         [0.0156, 0.3100],\n",
            "         ...,\n",
            "         [1.9766, 0.3100],\n",
            "         [1.9844, 0.3100],\n",
            "         [1.9922, 0.3100]],\n",
            "\n",
            "        [[0.0000, 0.0000],\n",
            "         [0.0078, 0.0000],\n",
            "         [0.0156, 0.0000],\n",
            "         ...,\n",
            "         [1.9766, 0.0000],\n",
            "         [1.9844, 0.0000],\n",
            "         [1.9922, 0.0000]]], grad_fn=<CatBackward0>)\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "File \u001b[0;32m<timed eval>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "Input \u001b[0;32mIn [253]\u001b[0m, in \u001b[0;36mPhysicsInformedNN.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/lbfgs.py:312\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    309\u001b[0m state\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_iter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# evaluate initial f(x) and df/dx\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m orig_loss \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(orig_loss)\n\u001b[1;32m    314\u001b[0m current_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[0;32mIn [253]\u001b[0m, in \u001b[0;36mPhysicsInformedNN.loss_func\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_func\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 96\u001b[0m     u_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet_u\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_u\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt_u\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m     f_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet_f(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx_f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt_f)\n\u001b[1;32m     98\u001b[0m     loss_u \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mu \u001b[38;5;241m-\u001b[39m u_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
            "Input \u001b[0;32mIn [253]\u001b[0m, in \u001b[0;36mPhysicsInformedNN.net_u\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     59\u001b[0m integral_u_dx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnn(t_by_x)\u001b[38;5;241m*\u001b[39mdelta_x, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     60\u001b[0m second_term \u001b[38;5;241m=\u001b[39m integral_u_dx \u001b[38;5;241m/\u001b[39m volume_x\n\u001b[0;32m---> 62\u001b[0m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m c_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull(x\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc)\n\u001b[1;32m     64\u001b[0m third_term \u001b[38;5;241m=\u001b[39m c_tensor \u001b[38;5;241m/\u001b[39m volume_x\n",
            "\u001b[0;31mSystemExit\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "8oBHMKejBPcI",
        "outputId": "1b250678-d2a1-435f-bb9f-10c172f85356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error u: 2.914023e-03\n",
            "Error c: 7.273114e-02\n"
          ]
        }
      ],
      "source": [
        "u_pred, f_pred, c_pred = model.predict(X_star)\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "\n",
        "# c_pred is prediction of c on test set, c_test is groundtruth of c on test set\n",
        "c_true = np.mean(np.sum(Exact*1/128, axis=1))\n",
        "\n",
        "error_c = abs(c_pred-c_true)\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error c: %e' % (error_c))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
