{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtSFXNgoBPb-"
      },
      "source": [
        "# Attribute\n",
        "\n",
        "**Original Work**: *Maziar Raissi, Paris Perdikaris, and George Em Karniadakis*\n",
        "\n",
        "**Github Repo** : https://github.com/maziarraissi/PINNs\n",
        "\n",
        "**Link:** https://github.com/maziarraissi/PINNs/tree/master/appendix/continuous_time_identification%20(Burgers)\n",
        "\n",
        "@article{raissi2017physicsI,\n",
        "  title={Physics Informed Deep Learning (Part I): Data-driven Solutions of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10561},\n",
        "  year={2017}\n",
        "}\n",
        "\n",
        "@article{raissi2017physicsII,\n",
        "  title={Physics Informed Deep Learning (Part II): Data-driven Discovery of Nonlinear Partial Differential Equations},\n",
        "  author={Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},\n",
        "  journal={arXiv preprint arXiv:1711.10566},\n",
        "  year={2017}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nty94OqjBPcC"
      },
      "source": [
        "## Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "vCDHrQyyBPcD"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '../Utilities/')\n",
        "\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from pyDOE import lhs\n",
        "import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "from scipy.interpolate import griddata\n",
        "# from plotting import newfig, savefig\n",
        "# from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "# import matplotlib.gridspec as gridspec\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GUNvzj-ZBPcE"
      },
      "outputs": [],
      "source": [
        "# CUDA support\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQT43UVTBPcF"
      },
      "source": [
        "## Physics-informed Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "USfxO5KqBPcF"
      },
      "outputs": [],
      "source": [
        "# the deep neural network\n",
        "class DNN(torch.nn.Module):\n",
        "    def __init__(self, layers):\n",
        "        super(DNN, self).__init__()\n",
        "\n",
        "        # parameters\n",
        "        self.depth = len(layers) - 1\n",
        "\n",
        "        # set up layer order dict\n",
        "        self.activation = torch.nn.Tanh\n",
        "\n",
        "        layer_list = list()\n",
        "        for i in range(self.depth - 1):\n",
        "            layer_list.append(\n",
        "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1]))\n",
        "            )\n",
        "            layer_list.append(('activation_%d' % i, self.activation()))\n",
        "\n",
        "        layer_list.append(\n",
        "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
        "        )\n",
        "        layerDict = OrderedDict(layer_list)\n",
        "\n",
        "        # deploy layers\n",
        "        self.layers = torch.nn.Sequential(layerDict)\n",
        "\n",
        "        for module in self.layers.modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                torch.nn.init.xavier_normal_(module.weight)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layers(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gUZx2DzjBPcF"
      },
      "outputs": [],
      "source": [
        "# the physics-guided neural network\n",
        "class PhysicsInformedNN():\n",
        "    def __init__(self, X_u, u, X_f, c, all_x, all_t, layers, lb, ub, nu):\n",
        "\n",
        "        # boundary conditions\n",
        "        self.lb = torch.tensor(lb).float().to(device)\n",
        "        self.ub = torch.tensor(ub).float().to(device)\n",
        "\n",
        "        # data\n",
        "        self.x_u = torch.tensor(X_u[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_u = torch.tensor(X_u[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.x_f = torch.tensor(X_f[:, 0:1], requires_grad=True).float().to(device)\n",
        "        self.t_f = torch.tensor(X_f[:, 1:2], requires_grad=True).float().to(device)\n",
        "        self.all_x = torch.tensor(all_x, requires_grad=True).float().to(device)\n",
        "        self.all_t = torch.tensor(all_t, requires_grad=True).float().to(device)\n",
        "        self.u = torch.tensor(u).float().to(device)\n",
        "        self.delta_x = 1/128\n",
        "        self.c = c\n",
        "\n",
        "        self.layers = layers\n",
        "        self.nu = nu\n",
        "\n",
        "        # deep neural networks\n",
        "        self.dnn = DNN(layers).to(device)\n",
        "\n",
        "        # optimizers: using the same settings\n",
        "        self.optimizer = torch.optim.LBFGS(\n",
        "            self.dnn.parameters(),\n",
        "            lr=1.0,\n",
        "            max_iter=50000,\n",
        "            max_eval=50000,\n",
        "            history_size=50,\n",
        "            tolerance_grad=1e-5,\n",
        "            tolerance_change=1.0 * np.finfo(float).eps,\n",
        "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
        "        )\n",
        "\n",
        "        self.iter = 0\n",
        "\n",
        "    def net_u(self, x, t):\n",
        "\n",
        "        u = self.dnn(torch.cat([x, t], dim=1))\n",
        "\n",
        "        volume_x = 2\n",
        "        delta_x = 1/128\n",
        "\n",
        "        mesh_t, mesh_x = torch.meshgrid([t.squeeze(1), self.all_x.squeeze(1)], indexing='ij')\n",
        "        t_by_x = torch.concat((mesh_x.unsqueeze(2), mesh_t.unsqueeze(2)), dim=-1)\n",
        "\n",
        "        integral_u_dx = torch.sum(self.dnn(t_by_x)*delta_x, dim=1)\n",
        "        second_term = integral_u_dx / volume_x\n",
        "\n",
        "        c_tensor = torch.full(x.shape, self.c)\n",
        "        third_term = c_tensor / volume_x\n",
        "        return u - second_term + third_term\n",
        "        return u\n",
        "\n",
        "    def net_f(self, x, t):\n",
        "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
        "        u = self.net_u(x, t)\n",
        "        u_t = torch.autograd.grad(\n",
        "            u, t,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_x = torch.autograd.grad(\n",
        "            u, x,\n",
        "            grad_outputs=torch.ones_like(u),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_xx = torch.autograd.grad(\n",
        "            u_x, x,\n",
        "            grad_outputs=torch.ones_like(u_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "        u_xxx = torch.autograd.grad(\n",
        "            u_xx, x,\n",
        "            grad_outputs=torch.ones_like(u_x),\n",
        "            retain_graph=True,\n",
        "            create_graph=True\n",
        "        )[0]\n",
        "\n",
        "        f = u_t + u * u_x + self.nu * u_xxx\n",
        "        return f\n",
        "\n",
        "    def loss_func(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        u_pred = self.net_u(self.x_u, self.t_u)\n",
        "        f_pred = self.net_f(self.x_f, self.t_f)\n",
        "        loss_u = torch.mean((self.u - u_pred) ** 2)\n",
        "        loss_f = torch.mean(f_pred ** 2)\n",
        "\n",
        "        loss = loss_u + loss_f\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            print(\n",
        "                'Iter %d, Loss: %.5e, Loss_u: %.5e, Loss_f: %.5e' % (self.iter, loss.item(), loss_u.item(), loss_f.item())\n",
        "            )\n",
        "        return loss\n",
        "\n",
        "    def train(self):\n",
        "        self.dnn.train()\n",
        "\n",
        "        # Backward and optimize\n",
        "        self.optimizer.step(self.loss_func)\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
        "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
        "\n",
        "        self.dnn.eval()\n",
        "        u = self.net_u(x, t)\n",
        "        f = self.net_f(x, t)\n",
        "        c = torch.sum(u*self.delta_x)\n",
        "        u = u.detach().cpu().numpy()\n",
        "        f = f.detach().cpu().numpy()\n",
        "        c = c.detach().cpu().numpy()\n",
        "        return u, f, c"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqfVFb6aBPcG"
      },
      "source": [
        "## Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fDvxZz4tBPcH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.0999881561168933e-16\n"
          ]
        }
      ],
      "source": [
        "nu = 0.0025\n",
        "noise = 0.0\n",
        "\n",
        "N_u = 100\n",
        "N_f = 10000\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "\n",
        "data = scipy.io.loadmat('../data/KdV.mat')\n",
        "\n",
        "# Pinn paper state data\n",
        "t = data['tt'].flatten()[:,None][:100]\n",
        "x = data['x'].flatten()[:,None][::2]\n",
        "u = np.real(data['uu'][::2, :100]).T # (100, 256)\n",
        "\n",
        "X, T = np.meshgrid(x,t)\n",
        "\n",
        "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "u_star = u.flatten()[:,None]\n",
        "\n",
        "# Domain bounds\n",
        "lb = X_star.min(0)\n",
        "ub = X_star.max(0)\n",
        "\n",
        "xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "uu1 = u[0:1,:].T\n",
        "xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "uu2 = u[:,0:1]\n",
        "xx3 = np.hstack((X[:,-1:], T[:,-1:]))\n",
        "uu3 = u[:,-1:]\n",
        "\n",
        "\n",
        "X_u_train = np.vstack([xx1, xx2, xx3])\n",
        "X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "c = np.mean(np.sum(u*1/128, axis=1))\n",
        "\n",
        "X_f_train = np.vstack((X_f_train, X_u_train))\n",
        "u_train = np.vstack([uu1, uu2, uu3])\n",
        "\n",
        "idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "X_u_train = X_u_train[idx, :]\n",
        "u_train = u_train[idx,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lp3TK1EBPcH"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "X_vyIhUnBPcH"
      },
      "outputs": [],
      "source": [
        "model = PhysicsInformedNN(X_u_train, u_train, X_f_train, c, x, t, layers, lb, ub, nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0SiaQjMRBPcH",
        "outputId": "5b995e96-2aaa-410a-a703-3f7e5f7587f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iter 100, Loss: 4.59254e-02, Loss_u: 3.67370e-02, Loss_f: 9.18838e-03\n",
            "Iter 200, Loss: 2.35196e-02, Loss_u: 1.60024e-02, Loss_f: 7.51719e-03\n",
            "Iter 300, Loss: 1.16608e-02, Loss_u: 7.73449e-03, Loss_f: 3.92630e-03\n",
            "Iter 400, Loss: 7.71783e-03, Loss_u: 4.63818e-03, Loss_f: 3.07965e-03\n",
            "Iter 500, Loss: 6.02060e-03, Loss_u: 3.30896e-03, Loss_f: 2.71164e-03\n",
            "Iter 600, Loss: 4.81127e-03, Loss_u: 2.43441e-03, Loss_f: 2.37686e-03\n",
            "Iter 700, Loss: 3.92225e-03, Loss_u: 2.08654e-03, Loss_f: 1.83571e-03\n",
            "Iter 800, Loss: 3.35958e-03, Loss_u: 1.73721e-03, Loss_f: 1.62237e-03\n",
            "Iter 900, Loss: 2.82365e-03, Loss_u: 1.38363e-03, Loss_f: 1.44002e-03\n",
            "Iter 1000, Loss: 2.31172e-03, Loss_u: 1.20337e-03, Loss_f: 1.10835e-03\n",
            "Iter 1100, Loss: 1.97560e-03, Loss_u: 1.07534e-03, Loss_f: 9.00266e-04\n",
            "Iter 1200, Loss: 1.71728e-03, Loss_u: 9.04736e-04, Loss_f: 8.12542e-04\n",
            "Iter 1300, Loss: 1.55914e-03, Loss_u: 8.23354e-04, Loss_f: 7.35787e-04\n",
            "Iter 1400, Loss: 1.41778e-03, Loss_u: 7.52972e-04, Loss_f: 6.64804e-04\n",
            "Iter 1500, Loss: 1.25589e-03, Loss_u: 6.36390e-04, Loss_f: 6.19499e-04\n",
            "Iter 1600, Loss: 1.14725e-03, Loss_u: 6.08460e-04, Loss_f: 5.38792e-04\n",
            "Iter 1700, Loss: 1.06604e-03, Loss_u: 5.62986e-04, Loss_f: 5.03058e-04\n",
            "Iter 1800, Loss: 1.00201e-03, Loss_u: 5.62935e-04, Loss_f: 4.39071e-04\n",
            "Iter 1900, Loss: 9.40955e-04, Loss_u: 4.99400e-04, Loss_f: 4.41555e-04\n",
            "Iter 2000, Loss: 8.66893e-04, Loss_u: 4.88345e-04, Loss_f: 3.78547e-04\n",
            "Iter 2100, Loss: 8.02146e-04, Loss_u: 4.57182e-04, Loss_f: 3.44963e-04\n",
            "Iter 2200, Loss: 7.33573e-04, Loss_u: 4.15839e-04, Loss_f: 3.17734e-04\n",
            "Iter 2300, Loss: 6.82497e-04, Loss_u: 3.83209e-04, Loss_f: 2.99288e-04\n",
            "Iter 2400, Loss: 6.48682e-04, Loss_u: 3.69271e-04, Loss_f: 2.79411e-04\n",
            "Iter 2500, Loss: 6.27498e-04, Loss_u: 3.58247e-04, Loss_f: 2.69250e-04\n",
            "Iter 2600, Loss: 5.94801e-04, Loss_u: 3.36997e-04, Loss_f: 2.57803e-04\n",
            "Iter 2700, Loss: 5.60443e-04, Loss_u: 3.21178e-04, Loss_f: 2.39265e-04\n",
            "Iter 2800, Loss: 5.26152e-04, Loss_u: 2.88135e-04, Loss_f: 2.38016e-04\n",
            "Iter 2900, Loss: 4.95353e-04, Loss_u: 2.77062e-04, Loss_f: 2.18292e-04\n",
            "Iter 3000, Loss: 4.66927e-04, Loss_u: 2.75565e-04, Loss_f: 1.91362e-04\n",
            "Iter 3100, Loss: 5.63110e-04, Loss_u: 2.53193e-04, Loss_f: 3.09916e-04\n",
            "Iter 3200, Loss: 4.22505e-04, Loss_u: 2.43054e-04, Loss_f: 1.79450e-04\n",
            "Iter 3300, Loss: 4.06937e-04, Loss_u: 2.29872e-04, Loss_f: 1.77065e-04\n",
            "Iter 3400, Loss: 3.91403e-04, Loss_u: 2.18320e-04, Loss_f: 1.73082e-04\n",
            "Iter 3500, Loss: 3.76258e-04, Loss_u: 2.11126e-04, Loss_f: 1.65132e-04\n",
            "Iter 3600, Loss: 3.63263e-04, Loss_u: 2.00794e-04, Loss_f: 1.62469e-04\n",
            "Iter 3700, Loss: 3.48010e-04, Loss_u: 2.01693e-04, Loss_f: 1.46317e-04\n",
            "Iter 3800, Loss: 3.31135e-04, Loss_u: 1.79836e-04, Loss_f: 1.51299e-04\n",
            "Iter 3900, Loss: 3.19158e-04, Loss_u: 1.60949e-04, Loss_f: 1.58209e-04\n",
            "Iter 4000, Loss: 3.07098e-04, Loss_u: 1.50656e-04, Loss_f: 1.56442e-04\n",
            "Iter 4100, Loss: 2.96469e-04, Loss_u: 1.44218e-04, Loss_f: 1.52251e-04\n",
            "Iter 4200, Loss: 2.80967e-04, Loss_u: 1.35243e-04, Loss_f: 1.45723e-04\n",
            "Iter 4300, Loss: 2.74952e-04, Loss_u: 1.32744e-04, Loss_f: 1.42207e-04\n",
            "Iter 4400, Loss: 2.69053e-04, Loss_u: 1.29666e-04, Loss_f: 1.39387e-04\n",
            "Iter 4500, Loss: 2.63672e-04, Loss_u: 1.24798e-04, Loss_f: 1.38874e-04\n",
            "Iter 4600, Loss: 2.58899e-04, Loss_u: 1.25270e-04, Loss_f: 1.33629e-04\n",
            "Iter 4700, Loss: 2.52998e-04, Loss_u: 1.24111e-04, Loss_f: 1.28887e-04\n",
            "Iter 4800, Loss: 2.46841e-04, Loss_u: 1.22401e-04, Loss_f: 1.24439e-04\n",
            "Iter 4900, Loss: 2.38330e-04, Loss_u: 1.18566e-04, Loss_f: 1.19764e-04\n",
            "Iter 5000, Loss: 2.29012e-04, Loss_u: 1.12819e-04, Loss_f: 1.16193e-04\n",
            "Iter 5100, Loss: 2.21925e-04, Loss_u: 1.09029e-04, Loss_f: 1.12896e-04\n",
            "Iter 5200, Loss: 2.17032e-04, Loss_u: 1.06453e-04, Loss_f: 1.10579e-04\n",
            "Iter 5300, Loss: 2.09787e-04, Loss_u: 1.01159e-04, Loss_f: 1.08627e-04\n",
            "Iter 5400, Loss: 2.01696e-04, Loss_u: 9.65892e-05, Loss_f: 1.05107e-04\n",
            "Iter 5500, Loss: 1.97277e-04, Loss_u: 9.04324e-05, Loss_f: 1.06844e-04\n",
            "Iter 5600, Loss: 1.92092e-04, Loss_u: 9.02923e-05, Loss_f: 1.01799e-04\n",
            "Iter 5700, Loss: 1.89306e-04, Loss_u: 8.93389e-05, Loss_f: 9.99671e-05\n",
            "Iter 5800, Loss: 1.85316e-04, Loss_u: 8.73293e-05, Loss_f: 9.79871e-05\n",
            "Iter 5900, Loss: 1.82391e-04, Loss_u: 8.71212e-05, Loss_f: 9.52701e-05\n",
            "Iter 6000, Loss: 1.79338e-04, Loss_u: 8.55956e-05, Loss_f: 9.37423e-05\n",
            "Iter 6100, Loss: 1.75683e-04, Loss_u: 8.25071e-05, Loss_f: 9.31757e-05\n",
            "Iter 6200, Loss: 1.71901e-04, Loss_u: 7.86467e-05, Loss_f: 9.32540e-05\n",
            "Iter 6300, Loss: 1.67075e-04, Loss_u: 7.69810e-05, Loss_f: 9.00935e-05\n",
            "Iter 6400, Loss: 1.63145e-04, Loss_u: 7.45245e-05, Loss_f: 8.86204e-05\n",
            "Iter 6500, Loss: 1.60524e-04, Loss_u: 7.37962e-05, Loss_f: 8.67273e-05\n",
            "Iter 6600, Loss: 1.57480e-04, Loss_u: 7.48701e-05, Loss_f: 8.26100e-05\n",
            "Iter 6700, Loss: 1.53003e-04, Loss_u: 7.37041e-05, Loss_f: 7.92990e-05\n",
            "Iter 6800, Loss: 1.48958e-04, Loss_u: 7.17681e-05, Loss_f: 7.71901e-05\n",
            "Iter 6900, Loss: 1.46252e-04, Loss_u: 7.00527e-05, Loss_f: 7.61990e-05\n",
            "Iter 7000, Loss: 1.43029e-04, Loss_u: 6.46700e-05, Loss_f: 7.83586e-05\n",
            "Iter 7100, Loss: 1.40049e-04, Loss_u: 6.11063e-05, Loss_f: 7.89426e-05\n",
            "Iter 7200, Loss: 1.35909e-04, Loss_u: 5.87163e-05, Loss_f: 7.71929e-05\n",
            "Iter 7300, Loss: 1.31553e-04, Loss_u: 5.41862e-05, Loss_f: 7.73672e-05\n",
            "Iter 7400, Loss: 1.28660e-04, Loss_u: 5.43782e-05, Loss_f: 7.42816e-05\n",
            "Iter 7500, Loss: 1.25316e-04, Loss_u: 5.50036e-05, Loss_f: 7.03123e-05\n",
            "Iter 7600, Loss: 1.23409e-04, Loss_u: 5.42586e-05, Loss_f: 6.91504e-05\n",
            "Iter 7700, Loss: 1.20820e-04, Loss_u: 5.26225e-05, Loss_f: 6.81971e-05\n",
            "Iter 7800, Loss: 1.18644e-04, Loss_u: 5.27134e-05, Loss_f: 6.59305e-05\n",
            "Iter 7900, Loss: 1.15918e-04, Loss_u: 5.29475e-05, Loss_f: 6.29706e-05\n",
            "Iter 8000, Loss: 1.14004e-04, Loss_u: 5.16607e-05, Loss_f: 6.23429e-05\n",
            "Iter 8100, Loss: 1.11785e-04, Loss_u: 5.08270e-05, Loss_f: 6.09578e-05\n",
            "Iter 8200, Loss: 1.09858e-04, Loss_u: 5.23880e-05, Loss_f: 5.74701e-05\n",
            "Iter 8300, Loss: 1.07111e-04, Loss_u: 5.24536e-05, Loss_f: 5.46575e-05\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m<timed eval>:1\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
            "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mPhysicsInformedNN.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdnn\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/lbfgs.py:426\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_directional_evaluate(closure, x, t, d)\n\u001b[0;32m--> 426\u001b[0m     loss, flat_grad, t, ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[43m_strong_wolfe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_init\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgtd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[1;32m    429\u001b[0m opt_cond \u001b[38;5;241m=\u001b[39m flat_grad\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m tolerance_grad\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/lbfgs.py:50\u001b[0m, in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     48\u001b[0m g \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39mclone(memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcontiguous_format)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# evaluate objective and gradient using initial step\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m f_new, g_new \u001b[38;5;241m=\u001b[39m \u001b[43mobj_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m ls_func_evals \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     52\u001b[0m gtd_new \u001b[38;5;241m=\u001b[39m g_new\u001b[38;5;241m.\u001b[39mdot(d)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/lbfgs.py:424\u001b[0m, in \u001b[0;36mLBFGS.step.<locals>.obj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobj_func\u001b[39m(x, t, d):\n\u001b[0;32m--> 424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_directional_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/optim/lbfgs.py:278\u001b[0m, in \u001b[0;36mLBFGS._directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_directional_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, closure, x, t, d):\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_grad(t, d)\n\u001b[0;32m--> 278\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    279\u001b[0m     flat_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_param(x)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mPhysicsInformedNN.loss_func\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m loss_f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(f_pred \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     97\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_u \u001b[38;5;241m+\u001b[39m loss_f\n\u001b[0;32m---> 99\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8oBHMKejBPcI",
        "outputId": "1b250678-d2a1-435f-bb9f-10c172f85356"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error u: 2.611875e-02\n",
            "Error c: 5.575175e-01\n"
          ]
        }
      ],
      "source": [
        "u_pred, f_pred, c_pred = model.predict(X_star)\n",
        "\n",
        "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
        "\n",
        "# c_pred is prediction of c on test set, c_test is groundtruth of c on test set\n",
        "c_true = np.mean(np.sum(u*1/128, axis=1))\n",
        "\n",
        "error_c = abs(c_pred-c_true)\n",
        "print('Error u: %e' % (error_u))\n",
        "print('Error c: %e' % (error_c))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
